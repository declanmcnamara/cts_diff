{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f8c8b9",
   "metadata": {},
   "source": [
    "# Score-Based Generative Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c430692",
   "metadata": {},
   "source": [
    "This repository is an implementation of [Song et al. (2020)](https://arxiv.org/pdf/2011.13456), and we try to reference equation numbers where possible. Code is built using JAX and [`equinox`](https://docs.kidger.site/equinox/). We credit the UNet architecture adapted from the [`sbgm` package](https://github.com/homerjed/sbgm), and adapt some of code therin for our implementation, as well as from [Patrick Kidger's tutorial](https://docs.kidger.site/equinox/examples/score_based_diffusion/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9c02bd",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608be71",
   "metadata": {},
   "source": [
    "Score-based generative modeling (SBGM) can be seen as a continuous-time reformulation and generalization of the original denoising diffusion probabilistic model (DDPM) paper. ([For an example implementation and tutorial of DDPM, see here](https://github.com/declanmcnamara/ddpm_mnist).)\n",
    "\n",
    "In SBGM, data are \"noised up\" according to an SDE. It can be shown that as the SDE evolves in time, the distribution tends towards a standard Gaussian, regardless of the original data point chosen.\n",
    "\n",
    "Generative modeling is performed by reversing this process through Anderson's theorem. We explain in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7720573e",
   "metadata": {},
   "source": [
    "#### Example: The Ornstein-Uhlenbeck SDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e35cb6",
   "metadata": {},
   "source": [
    "Suppose we have\n",
    "    \\begin{equation*}\n",
    "        d X_t  = m X_t dt + dW_t \\ \\ \\ \\textrm{[Ornstein-Uhlenbeck process]}\n",
    "    \\end{equation*}\n",
    "with $m=-1$, where $W_t$ denotes a standard Brownian motion. Then\n",
    "\\begin{equation*}\n",
    "    X_t = X_0 e^{-mt} + \\int_0^t e^{-(t-s)} dW_s\n",
    "\\end{equation*}\n",
    "for initial condition $X_0$.\n",
    "\n",
    "See Example 6.8 of Karatzas and Shreve, \\textit{Brownian Motion and Stochastic Calculus}}, for additional details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9fd30",
   "metadata": {},
   "source": [
    "For the same SDE above, if we have \n",
    "\\begin{equation*}\n",
    "        d X_t  = -X_t dt + dW_t \\ \\ \\ \\textrm{[Ornstein-Uhlenbeck process]}\n",
    "\\end{equation*}\n",
    "then \n",
    "\\begin{align*}\n",
    "    X_t | X_0 \\sim \\mathcal{N}(X_0 e^{-t}, \\frac{1}{2}(1-e^{-2t}))  \\\\\n",
    "    \\implies X_t | X_0 \\overset{d}{\\to} \\mathcal{N}(0, 1/2)\n",
    "\\end{align*}\n",
    "as $t \\to \\infty$.\n",
    "\n",
    "This is just a 1-dimensional example, but illustrates convergence to a chosen reference distribution. In the example above, the reference is $N(0, 1/2)$ instead of the standard Gaussian, but this can be resolved with rescalings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badbd56d",
   "metadata": {},
   "source": [
    "#### Anderson's Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce94a2",
   "metadata": {},
   "source": [
    "Let $X_0 \\sim p_0$ and thereafter evolve according to the SDE\n",
    "\\begin{equation*}\n",
    "    dX_t = f(X_t, t) dt + g(t) dW_t.\n",
    "\\end{equation*}\n",
    "\n",
    "Fix $T > 0$ and let $p_t$ denote the density of $X_t$ as defined above. Then with $U_0 \\sim p_T$ and\n",
    "\\begin{equation*}\n",
    "    dU_t = -\\left[ f(U_t, T-t) - g^2(T-t) \\nabla_x \\log p_{T-t}(U_t) \\right] dt + g(T-t) d\\tilde{W}_t,\n",
    "\\end{equation*}\n",
    "we have $X_t \\overset{d}{=} U_{T-t}$.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c1ef7",
   "metadata": {},
   "source": [
    "Anderson's theorem allows us to perform generative modeling provided we can approximate two quantities well:\n",
    "\n",
    "1. Sampling $U_0 \\sim p_T$\n",
    "2. Computing the score function $\\nabla_x \\log p_t(x)$ for any $t,x$.\n",
    "\n",
    "These cannot be done exactly. For #1, we will only have $p_T \\approx \\mathcal{N}(0, I)$, and we sample $U_0$ from this standard Gaussian as if the equality holds exactly. This is one form of approximation error. For #2, the score of the marginal distributions $\\nabla_x \\log p_t(x)$ are generally unknown, and must be approximated. Note that the conditional score $\\nabla_x \\log p_{t \\mid x_0}(x_t \\mid x_0)$ may be easy to compute (see the Ornstein-Uhlenbeck example above), but the unconditional (marginal) score function is the target to apply Anderson's theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd5a82",
   "metadata": {},
   "source": [
    "### Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d80668",
   "metadata": {},
   "source": [
    "SBGM (Song et al., 2020) fits a neural network to approximate the unconditional score. The network function $s_\\theta$ takes it two arguments, $x$ and $t$, and returns an approximation to $\\nabla_x \\log p_t(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbbf095",
   "metadata": {},
   "source": [
    "The objective function is (Eq. 7 of Song et al.)\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_t \\left[\\lambda(t) \\mathbb{E}_{x_0}\\mathbb{E}_{x_t \\mid x_0} \\left(||s_\\theta(x_t, t) - \\nabla_{x_t} p(x_t \\mid x_0)||_2^2 \\right)  \\right],\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\mathbb{E}_t$ is a an expectation over $t \\sim \\mathrm{Unif}[0,T]$\n",
    "- $\\lambda(t)$ is a weighting function (counterintuitively, this really implies that a Uniform [0,T] over $t$ is not the correct/ideal distribution to average over -- beyond our scope).\n",
    "- The expectation $\\mathbb{E}_{x_0}\\mathbb{E}_{x_t \\mid x_0}$ is approximated by Monte Carlo ancestral sampling of $x_0 \\sim p_{\\textrm{data}}$, then $x_t \\mid x_0$ according to the forward (noising) SDE.\n",
    "- The conditional score $\\nabla_{x_t} p(x_t \\mid x_0)$ is computed analytically.\n",
    "\n",
    "Note that $s_\\theta(x_t, t)$ does not receive knowledge about $x_0$; it must be fit to learn the unconditional score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53647b8d",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e637d58",
   "metadata": {},
   "source": [
    "Items that must be fixed a priori:\n",
    "\n",
    "1. The choice of the forward (noising) SDE, with a known limiting reference distribution.\n",
    "2. Weighting function $\\lambda(t)$.\n",
    "3. Network architecture $s_\\theta(x, t)$ that takes in data points and time values.\n",
    "\n",
    "\n",
    "Thereafter, the algorithm can proceed as:\n",
    "\n",
    "1. Draw a data point $x_0$.\n",
    "2. Draw a time $t \\sim \\mathrm{Unif}[0,T]$. \n",
    "3. Sample $x_t | x_0$ by simulating the forward SDE trajectory (ideally analytically rather than by Euler-Maruyama discretization, e.g. see Ornstein-Uhlenbeck above).\n",
    "4. Compute score $\\nabla_{x_t} p(x_t \\mid x_0)$ of the draw.\n",
    "5. Compute predicted score $s_\\theta(x_t, t)$.\n",
    "6. Compute gradient of $\\lambda(t) \\cdot ||s_\\theta(x_t, t) - \\nabla_{x_t} p(x_t \\mid x_0)||_2^2$ w/r/t $\\theta$.\n",
    "7. Update $\\theta$ along the negative gradient direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9402174c",
   "metadata": {},
   "source": [
    "Of course, the algorithm above can be performed across batches of data points. Note the two types of scores/gradients floating around: the score function $\\nabla_x \\log p_t(x)$ is a part of the objective function in $\\theta$, the network parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa693a2",
   "metadata": {},
   "source": [
    "### Implementation - Forward SDE + Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa4cb8",
   "metadata": {},
   "source": [
    "First, we define a `ForwardSDE` module. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1cd921",
   "metadata": {},
   "source": [
    "We try to use notation that mirrors that from Song et al. (2020), and reference equations where possible. The forward SDE we implement is the variance preserved (VP) SDE (Eq. 25 of Song et al.). Where $\\beta$ is referenced in code, it is meant to reference this notation. The SDE is\n",
    "\n",
    "$$\n",
    "dx_t = -\\frac{1}{2} \\beta(t) x_t dt + \\sqrt{\\beta(t)} dW_t\n",
    "$$\n",
    "\n",
    "This SDE can be shown to tend toward the standard Gaussian as $t$ grows large (Eq. 29). Below, `beta_module` defines a function $\\beta(t)$ that defines the forward process above allows for evaluation of the function $\\beta(t)$ and its integral efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da157cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import partial\n",
    "from typing import Callable, Optional, Self, Sequence, Tuple, Union\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from jaxtyping import Array, Key\n",
    "\n",
    "class ForwardSDE(eqx.Module):\n",
    "    betas: Array\n",
    "    beta_module: eqx.Module\n",
    "    dt: float\n",
    "\n",
    "    def __init__(self, betas: Array, dt: float = 0.01):\n",
    "        \"\"\"\n",
    "        Construct an SDE.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dt = dt\n",
    "        self.betas = betas\n",
    "        self.beta_module = (\n",
    "            BetaIntegralDefinedScheduler()\n",
    "        )\n",
    "\n",
    "    def forward_dist(self, t, x0):\n",
    "        \"\"\"Return the distribution of x_t conditional on x_0.\n",
    "\n",
    "        Form of Eq. (29) of SBGM.\n",
    "        \"\"\"\n",
    "        beta_int = self.beta_module.beta_int(t)\n",
    "        mu = x0 * jnp.exp((-1 / 2) * beta_int)\n",
    "        sig_sq = 1.0 - jnp.exp((-1 * beta_int))\n",
    "        scale = jnp.sqrt(sig_sq)\n",
    "        xt_dist = dist.Normal(mu, scale)\n",
    "        return xt_dist\n",
    "\n",
    "    def forward_sample(self, t, x0, key):\n",
    "        xt_dist = self.forward_dist(t, x0)\n",
    "        return xt_dist.sample(key)\n",
    "\n",
    "    def forward_sample_rparam(self, t, x0, key):\n",
    "        beta_int = self.beta_module.beta_int(t)\n",
    "        mu = x0 * jnp.exp((-1 / 2) * beta_int)\n",
    "        sig_sq = 1.0 - jnp.exp((-1 * beta_int))\n",
    "        scale = jnp.sqrt(sig_sq)\n",
    "        epsilon = jr.normal(key, x0.shape)\n",
    "        return mu, scale, epsilon\n",
    "\n",
    "    def marginal_log_prob(self, xt, t, x0):\n",
    "        xt_dist = self.forward_dist(t, x0)\n",
    "        return xt_dist.log_prob(xt).sum()\n",
    "\n",
    "    def f(self, x, t):\n",
    "        \"\"\"Define quantity $f(x, t)$ in Eq. (15) of SBGM.\"\"\"\n",
    "        return (-1 / 2) * self.beta_module.beta(t) * x\n",
    "\n",
    "    def G(self, x, t):\n",
    "        \"\"\"Define quantity $G(x,t)$ in Eq. (15) of SBGM.\n",
    "\n",
    "        NOTE: We do not explicitly return G(x,t) as a matrix.\n",
    "        This allows us to simply keep images in their original shape for\n",
    "        ease rather than dealing with reshaping, matrix multiplication, etc.\n",
    "        \"\"\"\n",
    "        value = math.sqrt(self.beta_module.beta(t))\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b237e6",
   "metadata": {},
   "source": [
    "We provide equation number references where useful in the comments. The key functions to note are:\n",
    "\n",
    "- `forward_dist`, returns the distribution of $x_t$ given $x_0$ and a time $t$.\n",
    "- `marginal_log_prob`, returns $p_t(x_t \\mid x_0)$ given all of these quantities. \n",
    "\n",
    "Below, `marginal_log_prob` will be differentiated to get the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be70f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def score(xt, t, x0, forward_sde: ForwardSDE):\n",
    "    wrapped_grad_fn = eqx.filter_grad(forward_sde.marginal_log_prob)\n",
    "    return wrapped_grad_fn(xt, t, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41441b6c",
   "metadata": {},
   "source": [
    "The `eqx.filter_` notation conveniently discards any non-arrays for JIT compilation. The `score` function thus simply returns `jax.grad` applied to the `marginal_log_prob` function above, giving us $\\nabla_x \\log p_t(x \\mid x_0)$ as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6015ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def sample_time(key, t0: float, t1: float, n_sample: int):\n",
    "    t = jr.uniform(key, (n_sample,), minval=t0, maxval=t1 / n_sample)\n",
    "    t = t + (t1 / n_sample) * jnp.arange(n_sample)\n",
    "    return t\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def my_single_loss_fn(score_model, t, x0, forward_sde: ForwardSDE, key):\n",
    "    mu, scale, eps = forward_sde.forward_sample_rparam(t, x0, key)\n",
    "    xt_draw = mu + scale * eps\n",
    "    pred_score = score_model(t, xt_draw, key=key)\n",
    "    actual_score = score(xt_draw, t, x0, forward_sde)\n",
    "    weight = lambda t: 1 - jnp.exp(-forward_sde.beta_module.beta_int(t))\n",
    "    return weight(t) * jnp.square(pred_score - actual_score).sum()\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def my_batched_loss_fn(score_model, batch_x0, forward_sde, key):\n",
    "    time_key, sde_key = jr.split(key)\n",
    "    batch_size = batch_x0.shape[0]\n",
    "    t = sample_time(time_key, 0.0, 10.0, batch_size)\n",
    "    part_x0_t = partial(\n",
    "        my_single_loss_fn, score_model=score_model, forward_sde=forward_sde, key=sde_key\n",
    "    )\n",
    "    return jax.vmap(part_x0_t)(x0=batch_x0, t=t).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6682699",
   "metadata": {},
   "source": [
    "The functions above are standard, some code is adapted from the tutorials referenced above. The weight function $\\lambda(t)$ depends on some attributes of the forward SDE, but can be altered. \n",
    "\n",
    "The function `my_single_loss_fn` implements the algorithm outlined above. \n",
    "\n",
    "The function `my_batched_loss_fn` simply wraps the above to support batching via `vmap`. For this problem, we observed that we have set $[0,T]= [0,10]$ -- we had to experiment to get $T$ sufficiently large enough that $x_T$ is approximately distributed as standard Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def make_step(score_model, batch_x0, forward_sde, optimizer, opt_state, key):\n",
    "    time_key, loss_key = jr.split(key)\n",
    "    score_model = eqx.nn.inference_mode(score_model, False)\n",
    "\n",
    "    loss_value, grads = eqx.filter_value_and_grad(my_batched_loss_fn)(\n",
    "        score_model, batch_x0, forward_sde, loss_key\n",
    "    )\n",
    "    updates, opt_state = optimizer.update(\n",
    "        grads, opt_state, eqx.filter(score_model, eqx.is_array)\n",
    "    )\n",
    "    score_model = eqx.apply_updates(score_model, updates)\n",
    "    key = jr.split(time_key, 1)[0]  # new key\n",
    "    return score_model, opt_state, loss_value, key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc441e6d",
   "metadata": {},
   "source": [
    "Lastly, `make_step` performs an entire step of the training loop. Notice we now take a second gradient, that of the batched loss function, with respect to the parameters of the `score_model`, and take a gradient step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e08cea",
   "metadata": {},
   "source": [
    "### Implementation - Reverse SDE + Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d9558",
   "metadata": {},
   "source": [
    "The Reverse SDE class is implemented to allow sampling. It samples using Euler-Maruyama sampling of the reverse SDE using the already-trained score network to compute the drift term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseSDE(eqx.Module):\n",
    "    dt: float\n",
    "    score_model: eqx.Module\n",
    "    forward_sde: eqx.Module\n",
    "    base_dist: numpyro.distributions.Distribution\n",
    "    shape: tuple\n",
    "    epsilon: float\n",
    "\n",
    "    def __init__(self, dt, score_model, forward_sde):\n",
    "        \"\"\"\n",
    "        Construct an SDE.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dt = dt\n",
    "        self.score_model = score_model\n",
    "        self.forward_sde = forward_sde\n",
    "        self.shape = (1, 28, 28)\n",
    "        self.base_dist = dist.Normal(loc=jnp.zeros(self.shape))\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def tilde_f(self, x, t):\n",
    "        \"\"\"The reverse SDE drift coefficient.\n",
    "\n",
    "        Defined in terms of x, t as well as\n",
    "        f, G from the forward SDE, as well as the\n",
    "        the score function (replaced by a trained model\n",
    "        of the score function here.)\n",
    "        \"\"\"\n",
    "        fxt = self.forward_sde.f(x, t)\n",
    "        gxt = self.forward_sde.G(x, t)  # scalar\n",
    "        score = self.score_model(t, x)\n",
    "        return fxt - (gxt**2) * score\n",
    "\n",
    "    def tilde_G(self, x, t):\n",
    "        \"\"\"Reverse time scale coefficient; same as forward.\"\"\"\n",
    "        return self.forward_sde.G(x, t)\n",
    "\n",
    "    def sample(self, key):\n",
    "        key = jr.split(key)[0]\n",
    "        x1 = self.base_dist.sample(key=key)\n",
    "        time_grid = jnp.arange(\n",
    "            start=10.0, stop=self.epsilon - self.dt, step=-1 * self.dt\n",
    "        )\n",
    "\n",
    "        curr_x = x1\n",
    "        for time in time_grid:\n",
    "            \"\"\"\n",
    "            Euler Maryuma iteration:\n",
    "            dx <- [f(x, t) - g^2(t) * score(x, t, q)] * dt + g(t) * sqrt(dt) * eps_t\n",
    "            x <- x + dx\n",
    "            t <- t + dt\n",
    "            \"\"\"\n",
    "\n",
    "            key = jr.split(key)[0]\n",
    "            time = jnp.array(time)\n",
    "\n",
    "            eps_t = jr.normal(key, self.shape)\n",
    "            drift = self.tilde_f(curr_x, time)\n",
    "            diffusion = self.tilde_G(curr_x, time)\n",
    "            next_x_mean = curr_x - drift * self.dt  # mu_x = x + drift * -step\n",
    "\n",
    "            curr_x = next_x_mean + diffusion * jnp.sqrt(self.dt) * eps_t\n",
    "\n",
    "        return next_x_mean\n",
    "\n",
    "    def sample_K(self, K, key):\n",
    "        key = jr.split(key)[0]\n",
    "        x1s = self.base_dist.sample(key, sample_shape=(K,))\n",
    "        time_grid = jnp.arange(start=10.0, stop=self.epsilon, step=-1 * self.dt)\n",
    "        curr_xs = x1s\n",
    "        for time in time_grid:\n",
    "            key = jr.split(key)[0]\n",
    "            time = jnp.array(time)\n",
    "\n",
    "            drift = partial(self.tilde_f, t=jnp.array(time))\n",
    "            noise = partial(self.tilde_G, t=jnp.array(time))\n",
    "            batch_drift = jax.vmap(drift)\n",
    "            batch_noise = jax.vmap(noise)\n",
    "\n",
    "            eps_t = jr.normal(key, x1s.shape)\n",
    "            drifts = batch_drift(curr_xs)\n",
    "            diffusions = batch_noise(curr_xs)\n",
    "            next_xs_means = curr_xs - drifts * self.dt  # mu_x = x + drift * -step\n",
    "\n",
    "            curr_xs = next_xs_means + diffusions[0] * jnp.sqrt(self.dt) * eps_t  # HACK\n",
    "\n",
    "        return next_xs_means\n",
    "\n",
    "    def plot_grid(self, nrow, ncol, key):\n",
    "        K = nrow * ncol\n",
    "        out = self.sample_K(K=K, key=key)\n",
    "\n",
    "        fig, ax = plt.subplots(nrow, ncol)\n",
    "        for i in range(nrow):\n",
    "            for j in range(ncol):\n",
    "                entry = ncol * i + j\n",
    "                this_image = out[entry][0]\n",
    "                ax[i, j].imshow(this_image)\n",
    "        plt.savefig(\"example_fig.png\")\n",
    "        plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f59b2",
   "metadata": {},
   "source": [
    "### Full Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41458595",
   "metadata": {},
   "source": [
    "We wrap everything into a training loop. Periodically, we use the reverse SDE to sample digits from noise to see how well we're doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efffca74",
   "metadata": {},
   "source": [
    "The code below sets up the experiment -- sets seeds, constructs a dataloader from the data, and instantiates the score model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "import os\n",
    "import hydra\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import torch\n",
    "import tqdm\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from network import UNet\n",
    "from samplers import ForwardSDE, ReverseSDE\n",
    "from utils import (\n",
    "    MNISTDataLoader,\n",
    "    load_model,\n",
    "    load_opt_state,\n",
    "    save_model,\n",
    "    save_opt_state,\n",
    ")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../conf\"):\n",
    "    cfg = compose(config_name=\"mnist\")\n",
    "    \n",
    "seed = cfg.seed\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = cfg.training.device\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device)\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "key = jr.key(seed)\n",
    "model_key, loader_key, train_key, sample_key = jr.split(key, 4)\n",
    "train_loader = MNISTDataLoader(key=loader_key, batch_size=cfg.training.batch_size)\n",
    "\n",
    "score_model = UNet(\n",
    "    data_shape=(1, 28, 28),\n",
    "    is_biggan=cfg.model.is_biggan,\n",
    "    dim_mults=cfg.model.dim_mults,\n",
    "    hidden_size=cfg.model.hidden_size,\n",
    "    heads=cfg.model.heads,\n",
    "    dim_head=cfg.model.dim_head,\n",
    "    dropout_rate=cfg.model.dropout_rate,\n",
    "    num_res_blocks=cfg.model.num_res_blocks,\n",
    "    attn_resolutions=cfg.model.attn_resolutions,\n",
    "    final_activation=cfg.model.final_activation,\n",
    "    q_dim=None,\n",
    "    a_dim=None,\n",
    "    key=model_key,\n",
    ")\n",
    "\n",
    "optimizer = hydra.utils.instantiate(cfg.optimizer)\n",
    "opt_state = optimizer.init(eqx.filter(score_model, eqx.is_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74cd946",
   "metadata": {},
   "source": [
    "Below, we construct the forward SDE module and begin the training process. The training process terminates after a number of steps specified in the config file. Every 10 epochs, we show some simulated digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_int_fcn = lambda t: t\n",
    "forward_sde = ForwardSDE(beta_int_fcn)\n",
    "\n",
    "counter = 0\n",
    "n_per_epoch = train_loader.n_batch\n",
    "epoch_losses = []\n",
    "epoch_idx = 0\n",
    "for batch_idx, (X_batch, y_batch) in enumerate(\n",
    "    tqdm.tqdm(train_loader.as_generator(), total=cfg.training.n_steps)\n",
    "):\n",
    "    score_model, opt_state, loss, train_key = make_step(\n",
    "        score_model, X_batch, forward_sde, optimizer, opt_state, train_key\n",
    "    )\n",
    "    epoch_losses.append(loss)\n",
    "    counter += 1\n",
    "    \n",
    "    if counter % n_per_epoch == 0:\n",
    "        \n",
    "        # Logging\n",
    "        avg_epoch_loss = jnp.array(epoch_losses).mean()\n",
    "        print(f\"Epoch {epoch_idx}: Avg. Loss {avg_epoch_loss}\")\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Log example image\n",
    "        if epoch_idx % 10 == 0:\n",
    "            score_model = eqx.nn.inference_mode(score_model, True)\n",
    "            reverse_sde = ReverseSDE(1e-1, score_model, forward_sde)\n",
    "            nrow = 5\n",
    "            ncol = 5\n",
    "            K = nrow * ncol\n",
    "            out = reverse_sde.sample_K(K=K, key=sample_key)\n",
    "\n",
    "            fig, ax = plt.subplots(nrow, ncol)\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    entry = ncol * i + j\n",
    "                    this_image = out[entry][0]\n",
    "                    ax[i, j].imshow(this_image)\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "            save_model(score_model, \"current_model.eqx\")\n",
    "\n",
    "            # Save optimiser state\n",
    "            save_opt_state(\n",
    "                optimizer,\n",
    "                opt_state,\n",
    "                i=epoch_idx * n_per_epoch,\n",
    "                filename=\"current_opt_state\",\n",
    "            )\n",
    "        epoch_idx += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a01fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_venv",
   "language": "python",
   "name": "master_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
